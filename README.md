# Neural Network From Scratch

This is an implementation of a simple multilayer neural network with one input, hidden, and output layer without the use of any existing deep learning libraries (e.g. TensorFlow, PyTorch). The network uses a sigmoid activation function and the weights are updated using gradient descent and momentum. The network is trained and tested on the MNIST handwritten digit dataset.
